{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "from scipy import signal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hjorth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjorth_mean(eeg_data):\n",
    "    \"\"\"\n",
    "    Calculate the Hjorth parameters of EEG data and return the mean values across channels for each epoch.\n",
    "    Args:\n",
    "        eeg_data (ndarray): EEG data of shape (number_of_epochs, number_of_channels, number_of_datapoints_per_epoch).\n",
    "    Returns:\n",
    "        mean_activity (ndarray): Mean activity parameter of shape (number_of_epochs,).\n",
    "        mean_mobility (ndarray): Mean mobility parameter of shape (number_of_epochs,).\n",
    "        mean_complexity (ndarray): Mean complexity parameter of shape (number_of_epochs,).\n",
    "    \"\"\"\n",
    "    n_epochs, n_channels, n_datapoints = eeg_data.shape\n",
    "    mean_activity = np.zeros((n_epochs,))\n",
    "    mean_mobility = np.zeros((n_epochs,))\n",
    "    mean_complexity = np.zeros((n_epochs,))\n",
    "    for i in range(n_epochs):\n",
    "        activity = 0\n",
    "        mobility = 0\n",
    "        complexity = 0\n",
    "        for j in range(n_channels):\n",
    "            signal = eeg_data[i, j, :]\n",
    "            diff1 = np.diff(signal)\n",
    "            diff2 = np.diff(signal, n=2)\n",
    "            var_zero = np.var(signal)\n",
    "            var_d1 = np.var(diff1)\n",
    "            var_d2 = np.var(diff2)\n",
    "            activity += var_zero\n",
    "            mobility += np.sqrt(var_d1 / var_zero)\n",
    "            complexity += np.sqrt(var_d2 / var_d1) / np.sqrt(var_d1 / var_zero)\n",
    "        mean_activity[i] = activity / n_channels\n",
    "        mean_mobility[i] = mobility / n_channels\n",
    "        mean_complexity[i] = complexity / n_channels\n",
    "    return mean_activity, mean_mobility, mean_complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kurtosis_feature(eeg_data):\n",
    "    num_epochs, num_channels, num_datapoints_per_epoch = eeg_data.shape\n",
    "    result = np.zeros(num_epochs)\n",
    "    for i in range(num_epochs):\n",
    "        epoch_data = eeg_data[i, :, :]\n",
    "        epoch_mean = np.mean(epoch_data, axis=1)\n",
    "        epoch_std = np.std(epoch_data, axis=1, ddof=1)\n",
    "        epoch_kurtosis = (\n",
    "            np.mean((epoch_data.T - epoch_mean) ** 4, axis=0) / epoch_std**4 - 3\n",
    "        )\n",
    "        result[i] = np.mean(epoch_kurtosis)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Fetures!\n",
    "### Approx Mean, Approx Std Deviation, Approx Energy, Detailed Mean, Detailed Std Deviation, Detailed Energy, Approx Entropy & Detailed Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_features(epoch):\n",
    "    num_epochs, num_channels, num_samples = epoch.shape\n",
    "    cA_mean = np.zeros((num_epochs, num_channels))\n",
    "    cA_std = np.zeros((num_epochs, num_channels))\n",
    "    cA_Energy = np.zeros((num_epochs, num_channels))\n",
    "    cD_mean = np.zeros((num_epochs, num_channels))\n",
    "    cD_std = np.zeros((num_epochs, num_channels))\n",
    "    cD_Energy = np.zeros((num_epochs, num_channels))\n",
    "    Entropy_D = np.zeros((num_epochs, num_channels))\n",
    "    Entropy_A = np.zeros((num_epochs, num_channels))\n",
    "    wfeatures = np.zeros((num_epochs, 7 * num_channels))\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(num_channels):\n",
    "            cA, cD = pywt.dwt(epoch[i, j, :], \"coif1\")\n",
    "            cA_mean[i, j] = np.mean(cA)\n",
    "            cA_std[i, j] = np.abs(np.std(cA))\n",
    "            cA_Energy[i, j] = np.sum(np.square(cA))\n",
    "            cD_mean[i, j] = np.mean(cD)\n",
    "            cD_std[i, j] = np.abs(np.std(cD))\n",
    "            cD_Energy[i, j] = np.sum(np.square(cD))\n",
    "            Entropy_D[i, j] = np.sum(np.square(cD) * np.log(np.square(cD)))\n",
    "            Entropy_A[i, j] = np.sum(np.square(cA) * np.log(np.square(cA)))\n",
    "\n",
    "    wfeatures[:, 0::7] = cA_mean\n",
    "    wfeatures[:, 1::7] = cA_std\n",
    "    wfeatures[:, 2::7] = cA_Energy\n",
    "    wfeatures[:, 3::7] = cD_mean\n",
    "    wfeatures[:, 4::7] = cD_std\n",
    "    wfeatures[:, 5::7] = cD_Energy\n",
    "    wfeatures[:, 6::7] = Entropy_D + Entropy_A\n",
    "\n",
    "    return wfeatures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Spectral Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxPwelch_epochs(epochs, Fs):\n",
    "  n_epochs, n_channels, n_samples_per_epoch = epochs.shape\n",
    "  BandF = [12, 30, 100]\n",
    "  PMax = np.zeros([n_epochs, n_channels, len(BandF) - 1])\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    for j in range(n_channels):\n",
    "      f, Psd = signal.welch(epochs[i, j, :], Fs)\n",
    "\n",
    "      if np.any(np.isnan(Psd)):\n",
    "        nonnan_values = Psd[~np.isnan(Psd)]\n",
    "        nan_average = np.mean(nonnan_values)\n",
    "        Psd[np.isnan(Psd)] = nan_average\n",
    "\n",
    "      for k in range(len(BandF) - 1):\n",
    "        fr = np.where((f > BandF[k]) & (f <= BandF[k + 1]))\n",
    "        PMax[i, j, k] = np.max(Psd[fr])\n",
    "\n",
    "  return PMax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preporcessed Data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for executing preprocessing.v1\n",
    "epoch_data = np.load(\"./cleaned_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for executing preprocessing.v2\n",
    "epoch_data = np.load(\"../codes-v2/filtered_data_v2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for executing preprocessing.v2 with all 29 subjects\n",
    "epoch_data = np.load(\"../codes-v2/filtered_full_data_v1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for executing preprocessing.v2 with four class classification\n",
    "epoch_data = np.load(\"../codes-v2/filtered_data_v3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for feature extraction of test-subjects\n",
    "subject_number = 'twofour'\n",
    "\n",
    "epoch_data = np.load(f\"../test-subjects/filtered-sub-{subject_number}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to extract features of single subject\n",
    "subjectid = '01'\n",
    "\n",
    "epoch_data = np.load(f'../codes-v2/filtered-sub-{subjectid}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 14, 500)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(epoch_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hjorth = hjorth_mean(epoch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hjorth_list = np.concatenate(\n",
    "    (hjorth[0][:, np.newaxis], hjorth[1][:, np.newaxis], hjorth[2][:, np.newaxis]),\n",
    "    axis=1,\n",
    ")\n",
    "display(hjorth_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(847,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(847,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(hjorth[0].shape, hjorth[1].shape, hjorth[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kurtosis = kurtosis_feature(epoch_data)\n",
    "display(kurtosis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 98)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wavelet = wavelet_features(epoch_data)\n",
    "display(wavelet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data):\n",
    "    flattened_data = data.reshape(data.shape[0], -1)\n",
    "    return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 14, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "psd = maxPwelch_epochs(epoch_data, 500)\n",
    "display(psd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "psd_2d = flatten(psd)\n",
    "display(psd_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to save features extracted for all 29 subjects\n",
    "np.save(\"../all-sub-features/hjorth_list.npy\", hjorth_list)\n",
    "np.save(\"../all-sub-features/kurtosis.npy\", kurtosis)\n",
    "np.save(\"../all-sub-features/wavelet.npy\", wavelet)\n",
    "np.save(\"../all-sub-features/psd_2d.npy\", psd_2d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding labels and building the final feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2882,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell for test-subjects\n",
    "labels = np.load(f\"../test-subjects/label-sub-{subject_number}.npy\")\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when executing preprocessing.v1\n",
    "labels = np.load(\"./labels.npy\")\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8391,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell when executing preprocessing.v2\n",
    "labels = np.load(\"../codes-v2/labels_v2.npy\")\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24354,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell when executing preprocessing.v2 with all 29 subjects\n",
    "labels = np.load(\"../codes-v2/labels_full_v1.npy\")\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16827,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell when executing preprocessing.v2 with four class\n",
    "labels = np.load(\"../codes-v2/labels_v3.npy\")\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell for single subject\n",
    "labels = np.load(f\"../codes-v2/label-sub-{subjectid}.npy\")\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Feature Vector for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load features extracted for all 29 subjects\n",
    "hjorth_list = np.load(\"../all-sub-features/hjorth_list.npy\")\n",
    "kurtosis = np.load(\"../all-sub-features/kurtosis.npy\")\n",
    "wavelet= np.load(\"../all-sub-features/wavelet.npy\")\n",
    "psd_2d = np.load(\"../all-sub-features/psd_2d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(847, 131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_vector = np.concatenate(\n",
    "    (\n",
    "      hjorth_list, \n",
    "      kurtosis[:, np.newaxis], \n",
    "      wavelet, \n",
    "      psd_2d,\n",
    "      labels[:, np.newaxis]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "display(feature_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when doing feature extration for test subjects\n",
    "np.save(f\"../test-subjects/wavelet-sub-{subject_number}.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when doing feature extraction specific to features for all 29 subjects\n",
    "np.save(\"../all-sub-features/rest-2-all.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when executing Preprocessing.v1\n",
    "np.save(\"labeled_feature_vector.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when doing feature extraction specific to features\n",
    "np.save(\"../codes-v2/labeled_feature_rest-2-all.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when executing preprocessing.v2 with four class\n",
    "np.save(\"../codes-v2/labeled_feature_v3.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when doing feature extration for single subject\n",
    "np.save(f\"../codes-v2/all-feature-sub-{subjectid}.npy\", feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_vector[:5, -1], feature_vector[-5:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(str):\n",
    "  switcher = {\n",
    "    \"rest\" : 0,\n",
    "    \"zerob\" : 1,\n",
    "    \"oneb\" : 2,\n",
    "    \"twob\" : 3,\n",
    "  }\n",
    "  return switcher.get(str, \"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(feature_vector, label_1, label_2):\n",
    "  l1 = encoder(label_1)\n",
    "  l2 =encoder(label_2)\n",
    "  feature_filter = feature_vector[np.logical_or(feature_vector[:, -1]==l1, feature_vector[:, -1]==l2)]\n",
    "  return feature_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8391, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "feature_filter = filter(feature_vector, \n",
    "                        \"rest\", \n",
    "                        \"twob\"\n",
    "                        )\n",
    "display(feature_filter.shape, feature_filter[:5, -1], feature_filter[-5:, -1])\n",
    "print(feature_filter[4150:4200, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when executing preprocessing.v2 with four class and filtering two classes\n",
    "np.save(\"../codes-v2/labeled_feature_v2.npy\", feature_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Feature Vector for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = np.concatenate(\n",
    "    (hjorth_list, kurtosis[:, np.newaxis], wavelet, psd_2d),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.91860190e-11 6.06506149e-01 1.95788105e+00 ... 4.22156961e-12\n",
      "  5.93556943e-13 2.53393562e-12]\n",
      " [4.27787090e-11 6.56674113e-01 1.95179283e+00 ... 7.45609870e-13\n",
      "  3.37528040e-13 3.89414158e-13]\n",
      " [4.07509746e-11 7.21396852e-01 1.76879551e+00 ... 8.25299746e-13\n",
      "  7.10077633e-13 2.79019337e-13]\n",
      " ...\n",
      " [3.92580840e-11 6.68710581e-01 1.99244923e+00 ... 4.73538717e-13\n",
      "  5.43931866e-13 2.58039600e-13]\n",
      " [4.09389814e-11 6.68757648e-01 2.02365791e+00 ... 4.73755447e-13\n",
      "  9.34644892e-13 5.62357637e-13]\n",
      " [4.22822163e-11 6.42625041e-01 2.12024910e+00 ... 2.96167865e-13\n",
      "  5.74304045e-13 3.07774333e-13]]\n"
     ]
    }
   ],
   "source": [
    "print(feature_vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16827, 130)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when applying DL with four class\n",
    "np.save(\"../codes-v2/dl_feature_c4.npy\", feature_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
