{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn. metrics import confusion_matrix, ConfusionMatrixDisplay , precision_score, recall_score, f1_score, r2_score, roc_curve, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell when checking metrics for single subject\n",
    "subjectid = '28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = np.load(f\"./data/hjorth-psd-wavelet-sub-{subjectid}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(837, 130)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(837,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = feature_matrix[:, -1]\n",
    "display(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print((np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(837, 129)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = feature_matrix[:, :-1]\n",
    "display(data.shape)\n",
    "for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if np.isnan(data[i,j]):\n",
    "                  display([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in fold.split(data, labels):\n",
    "    x_train, x_test, y_train, y_test = [data[i] for i in train_index], [data[i] for i in test_index], [labels[i] for i in train_index], [labels[i] for i in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cnf(cnf_matrix):\n",
    "  fig = px.imshow(cnf_matrix, \n",
    "                 color_continuous_scale='Blues')\n",
    "\n",
    "  fig.update_layout(\n",
    "      title=\"Confusion Matrix with Rest-case-labeled: 0, One-back-labeled: 1\",\n",
    "      xaxis_title=\"Actual Labels\",\n",
    "      yaxis_title=\"Predicted Labels\",\n",
    "      width= 700,\n",
    "      height=700,\n",
    "  )\n",
    "\n",
    "  fig.update_layout(\n",
    "      font=dict(\n",
    "          size=12\n",
    "      ),\n",
    "      xaxis = dict(\n",
    "          tick0=0,\n",
    "          dtick=1\n",
    "      ),\n",
    "      yaxis = dict(\n",
    "          tick0=0,\n",
    "          dtick=1\n",
    "      )\n",
    "  )\n",
    "\n",
    "  for i in range(len(cnf_matrix)):\n",
    "      for j in range(len(cnf_matrix)):\n",
    "          \n",
    "          if (cnf_matrix[i,j] >= 220):\n",
    "              color = 'white'\n",
    "          else: \n",
    "              color = 'black'\n",
    "\n",
    "          fig.add_annotation(text=str(cnf_matrix[i,j]), \n",
    "                            x=j, \n",
    "                            y=i, \n",
    "                            showarrow=False,\n",
    "                            font=dict(\n",
    "                                color=color,\n",
    "                                size=24,\n",
    "                            ) \n",
    "                            )\n",
    "\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curve Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(tpr, fpr):\n",
    "  \n",
    "  fig = px.line(x=fpr, \n",
    "                y=tpr,\n",
    "                )\n",
    "    \n",
    "  fig.add_scatter(x=[0,1], \n",
    "                  y=[0,1],\n",
    "                  line=dict(color='navy', dash='dash'),\n",
    "                  name=\"Guessing\"\n",
    "                  )\n",
    "\n",
    "  \n",
    "  fig.update_layout(\n",
    "      title='ROC Curve',\n",
    "      xaxis_title='False Positive Rate',\n",
    "      yaxis_title='True Positive Rate',\n",
    "      width=700,\n",
    "      height=500,\n",
    "  )\n",
    "\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot multiple ROC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot multiple ROC curevs on single plot\n",
    "\n",
    "def multi_roc_plot(models, x_train= x_train, y_train= y_train, x_test=x_test, y_test= y_test):\n",
    "  fpr_list = [] \n",
    "  tpr_list = []\n",
    "  legend_list = []\n",
    "  \n",
    "  for model in models:\n",
    "    # Training the models on x_train and y_train\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred_prob = model.predict_proba(X=x_test)         # y_pred_prob.shape = [len(x-test), 2], (prob(label0), prob(label1))\n",
    "\n",
    "    # Getting the FalsePositiveRate and TruePositveRates from plotting the ROC curve\n",
    "    fpr, tpr,_ = roc_curve(\n",
    "                          y_test, \n",
    "                          y_pred_prob[:,1],\n",
    "                          )\n",
    "    \n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "    index = str(model).find('(')\n",
    "    legend_list.append(str(model)[:index])\n",
    "\n",
    "  colors = px.colors.qualitative.Set1\n",
    "\n",
    "  fig = px.line()\n",
    "\n",
    "  for i, (tpr, fpr) in enumerate(zip(tpr_list, fpr_list)):\n",
    "    fig.add_scatter(\n",
    "                    x=fpr,\n",
    "                    y=tpr,\n",
    "                    mode='lines',\n",
    "                    line=dict(color= colors[i]),\n",
    "                    name= legend_list[i]\n",
    "                    )\n",
    "    \n",
    "  fig.add_scatter(x=[0,1], \n",
    "                  y=[0,1],\n",
    "                  line=dict(color='navy', dash='dash'),\n",
    "                  name=\"Guessing\"\n",
    "                  )\n",
    "  \n",
    "  fig.update_layout(\n",
    "      title='ROC Curve',\n",
    "      xaxis_title='False Positive Rate',\n",
    "      yaxis_title='True Positive Rate',\n",
    "      width=700,\n",
    "      height=500,\n",
    "  )\n",
    "\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics calculation function\n",
    "\n",
    "def metrics(model, labels, xTest=None, yTest=None):\n",
    "\n",
    "  if np.all(xTest == None):\n",
    "    # Using average of cross val score for accuracy\n",
    "    score = cross_val_score(model, \n",
    "                            data, \n",
    "                            labels,\n",
    "                            cv=10,\n",
    "                          )\n",
    "    \n",
    "    cross_val_acc = np.average(score)\n",
    "    cross_val_std = np.std(score)\n",
    "\n",
    "  # Training the model on x_train and y_train\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  if np.all(xTest == None):\n",
    "    # Getting the class-label predictions and class-label prediction probabilities from the trained model\n",
    "    model_predictions = model.predict(X=x_test)         # model_predictions.shape = [len(x-test)], (label(ith epoch))\n",
    "    y_pred_prob = model.predict_proba(X=x_test)         # y_pred_prob.shape = [len(x-test), 2], (prob(label0), prob(label1))\n",
    "  else:\n",
    "    # Getting the class-label predictions and class-label prediction probabilities for the test data\n",
    "    model_predictions = model.predict(X=xTest)         # model_predictions.shape = [len(x-test)], (label(ith epoch))\n",
    "    y_pred_prob = model.predict_proba(X=xTest)         # y_pred_prob.shape = [len(x-test), 2], (prob(label0), prob(label1))\n",
    "\n",
    "  if np.all(xTest == None):\n",
    "    # Building the Classification Report using the predictions as a dataframe without the accuracy column\n",
    "    classif_report = pd.DataFrame(classification_report(y_true=y_test, \n",
    "                                                        y_pred=model_predictions, \n",
    "                                                        output_dict=True,\n",
    "                                                        zero_division=0,\n",
    "                                                        )\n",
    "                                  ).drop(labels=\"accuracy\", axis=1).T.round(2)\n",
    "  else:\n",
    "    # Building the Classification Report using the predictions as a dataframe with the accuracy column\n",
    "    classif_report = pd.DataFrame(classification_report(y_true=yTest, \n",
    "                                                        y_pred=model_predictions, \n",
    "                                                        output_dict=True,\n",
    "                                                        zero_division=0,\n",
    "                                                        )\n",
    "                                  ).T.round(2)\n",
    "  if np.all(xTest == None):  \n",
    "    # Building the Confusion Matrix using the predicted class labels\n",
    "    cnf_matrix = confusion_matrix(y_true=y_test, y_pred=model_predictions)\n",
    "  else:\n",
    "    # Building the Confusion Matrix using the predicted class labels for test-subjects\n",
    "    cnf_matrix = confusion_matrix(y_true=yTest, y_pred=model_predictions)\n",
    "  \n",
    "  if len(np.unique(labels)) == 2:\n",
    "    if np.all(xTest == None):\n",
    "      # Getting the FalsePositiveRate and TruePositveRates from plotting the ROC curve\n",
    "      fpr, tpr, thresholds = roc_curve(\n",
    "                                      y_test, \n",
    "                                      y_pred_prob[:,1],\n",
    "                                      #  pos_label=3\n",
    "                                      )\n",
    "    else:\n",
    "      # Getting the FalsePositiveRate and TruePositveRates from plotting the ROC curve for test-subjects\n",
    "      fpr, tpr, thresholds = roc_curve(\n",
    "                                      yTest, \n",
    "                                      y_pred_prob[:,1],\n",
    "                                      #  pos_label=3\n",
    "                                      )\n",
    "\n",
    "  if len(np.unique(labels)) == 2:\n",
    "    if np.all(xTest == None):\n",
    "      # Calculating the Area under the ROC curve ie, AUC using class label prediction probabilities\n",
    "      auc = roc_auc_score(y_test, y_pred_prob[:,1])\n",
    "    else:\n",
    "      # Calculating the Area under the ROC curve ie, AUC using class label prediction probabilities for test-subjects\n",
    "      auc = roc_auc_score(yTest, y_pred_prob[:,1])\n",
    "\n",
    "  else:\n",
    "    micro_roc_auc_ovr = roc_auc_score(\n",
    "                                      y_test,\n",
    "                                      y_pred_prob,\n",
    "                                      multi_class=\"ovr\",\n",
    "                                      average=\"micro\",\n",
    "                                      )\n",
    "    \n",
    "    macro_roc_auc_ovr = roc_auc_score(\n",
    "                                      y_test,\n",
    "                                      y_pred_prob,\n",
    "                                      multi_class=\"ovr\",\n",
    "                                      average=\"macro\",\n",
    "                                      )\n",
    "    \n",
    "    macro_roc_auc_ovo = roc_auc_score(\n",
    "                                      y_test,\n",
    "                                      y_pred_prob,\n",
    "                                      multi_class=\"ovo\",\n",
    "                                      average=\"macro\",\n",
    "                                      )\n",
    "  if np.all(xTest == None):  \n",
    "    print(f\"Cross-val-mean-Accuracy: {100*cross_val_acc:.2f}\\n Cross-val-accuarcy-std: {cross_val_std}\")\n",
    "    print(f\"Cross-val-Accuracy: {100*cross_val_acc:.2f} +- {cross_val_std}\")\n",
    "\n",
    "  # display(classif_report)\n",
    "\n",
    "  # plot_cnf(cnf_matrix=cnf_matrix)\n",
    "\n",
    "  # if len(np.unique(labels)) == 2:\n",
    "  #   plot_roc(tpr=tpr,\n",
    "  #           fpr=fpr,\n",
    "  #           )\n",
    "  \n",
    "  # if len(np.unique(labels)) == 2:\n",
    "  #   print(f\"Area Under the ROC Curve (AUC): {100*auc:.2f}\")\n",
    "  \n",
    "  # else:\n",
    "  #   print(f\"\\nMicro-averaged One-vs-Rest ROC AUC score:{micro_roc_auc_ovr:.2f}\")\n",
    "  #   print(f\"\\nMacro-averaged One-vs-Rest ROC AUC score:{macro_roc_auc_ovr:.2f}\")\n",
    "  #   print(f\"\\nMacro-averaged One-vs-One ROC AUC score:{macro_roc_auc_ovo:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 97.86\n",
      " Cross-val-accuarcy-std: 0.024852414008836556\n",
      "Cross-val-Accuracy: 97.86 +- 0.024852414008836556\n"
     ]
    }
   ],
   "source": [
    "metrics(\n",
    "        xgb, \n",
    "        labels, \n",
    "        # xTest=xTest, \n",
    "        # yTest=yTest        \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 97.61\n",
      " Cross-val-accuarcy-std: 0.026639498630181717\n",
      "Cross-val-Accuracy: 97.61 +- 0.026639498630181717\n"
     ]
    }
   ],
   "source": [
    "metrics(etc, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 97.26\n",
      " Cross-val-accuarcy-std: 0.03239644500246336\n",
      "Cross-val-Accuracy: 97.26 +- 0.03239644500246336\n"
     ]
    }
   ],
   "source": [
    "metrics(rfc, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 61.55\n",
      " Cross-val-accuarcy-std: 0.2365874883875021\n",
      "Cross-val-Accuracy: 61.55 +- 0.2365874883875021\n"
     ]
    }
   ],
   "source": [
    "metrics(gnb, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', multi_class='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 62.86\n",
      " Cross-val-accuarcy-std: 0.24478633076571832\n",
      "Cross-val-Accuracy: 62.86 +- 0.24478633076571832\n"
     ]
    }
   ],
   "source": [
    "metrics(lr, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto',\n",
    "          probability=True,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 58.82\n",
      " Cross-val-accuarcy-std: 0.21315294004006452\n",
      "Cross-val-Accuracy: 58.82 +- 0.21315294004006452\n"
     ]
    }
   ],
   "source": [
    "metrics(svc, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val-mean-Accuracy: 71.70\n",
      " Cross-val-accuarcy-std: 0.2095872735690811\n",
      "Cross-val-Accuracy: 71.70 +- 0.2095872735690811\n"
     ]
    }
   ],
   "source": [
    "metrics(knn, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
